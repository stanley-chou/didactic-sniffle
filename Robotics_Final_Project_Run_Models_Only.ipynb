{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cru6TxnxkdYP"
   },
   "source": [
    "### Final Project : Deep Reinforcement Learning Extended\n",
    "\n",
    "### Stanley Chou RUID:193005065 , Anis Chihoub, Sunny Chen\n",
    "### Due: 12/16/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHFd0z4VQNRl"
   },
   "source": [
    "#1 Imports and Making sure the Environment Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tidXSzyAmSiz",
    "outputId": "79002f15-661a-4b60-c070-2c7b2c12916d"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# Import retro to play Street Fighter using a ROM\n",
    "import retro\n",
    "# Import time to slow down game\n",
    "import time\n",
    "import torch\n",
    "#import google drive from colab in order to avoid reuploading\n",
    "\n",
    "\n",
    "# See the different retro games\n",
    "#retro.data.list_games()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BH95BiLWzZ44"
   },
   "outputs": [],
   "source": [
    "#General Imports\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from gym.wrappers import Monitor\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fe_WmHzOnYoc"
   },
   "outputs": [],
   "source": [
    "#load our environment\n",
    "env = retro.make(game= 'StreetFighterIISpecialChampionEdition-Genesis')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhjPtNCISPcv"
   },
   "source": [
    "# Need to add preprocessing - grey scale the screen, get the frame differences currentscreen -last screen, possibly downsize the data\n",
    "#adjust the reward function to make it less sparse - and make it equal to the score of the game + the health you take away from the opponent, and minus the health you lose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LKWZobY1UcKb"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import the space shapes for the environment\n",
    "from gym import spaces\n",
    "from gym.spaces import MultiBinary, Box\n",
    "# Import numpy to calculate frame delta \n",
    "import numpy as np\n",
    "# Import opencv for grayscaling\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qMW_fuynU72P"
   },
   "outputs": [],
   "source": [
    "#The custom environment is necessarry for us to implement our own loss functions\n",
    "\n",
    "# Create custom environment \n",
    "class StreetFighter(gym.Env): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Specify action space and observation space \n",
    "        #The dimensions for the game environment itself is this in gray scale)\n",
    "        self.observation_space = Box(low=0, high=255, shape=(64, 64, 1), dtype=np.uint8)\n",
    "        #12 possible button presses on the fighting controller hence multibinary 12\n",
    "        self.action_space = MultiBinary(12)\n",
    "        # Startup and instance of the game \n",
    "        #the filter parameter forces the agent to only use valid button combinations e.g no passing [1,1,1,1,1,1,1,1,1,1,1,1] as the action vector and getting something \n",
    "        # (this example vector is equivalent to a person holding all the buttons on the arcade box controller and inputting all 4 directions at the same time which is impossible and does nothing so don't do it)\n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions = retro.Actions.FILTERED)\n",
    "    \n",
    "    def reset(self):\n",
    "        # Return the first frame \n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs) \n",
    "        #reset state so the previous frame in this case is the initial frame\n",
    "        self.previous_frame = obs \n",
    "        \n",
    "        # Create a attribute to hold the score delta \n",
    "        self.score = 0 \n",
    "        # Create a attribute to hold the Player Health delta\n",
    "        self.health = 176 \n",
    "        # Create a attribute to hold the Enemy Health delta The default health for characters at each round is 176\n",
    "        self.enemy_health = 176\n",
    "        self.matches_won = 0\n",
    "        self.enemy_matches_won = 0\n",
    "        return obs\n",
    "    \n",
    "    #Grayscaling and resizing the image to improve training time since we are poor and need computations to be easy\n",
    "    #Original image is 200x256x3 so we are turning it into a grayscale image of half the y dimension 128x128x1 (originally we did 100x128x1 but for some reason square images looked and did better)\n",
    "    def preprocess(self, observation): \n",
    "        # Grayscaling the frame\n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        # Resize (128x128)\n",
    "        resize = cv2.resize(gray, (64,64), interpolation=cv2.INTER_CUBIC)\n",
    "        # Add the channels value (128x128z1)\n",
    "        channels = np.reshape(resize, (64,64,1))\n",
    "        #print(channels.shape)\n",
    "        return channels\n",
    "    \n",
    "    def step(self, action): \n",
    "        # Take a step \n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        #preprocess the new game frame\n",
    "        obs = self.preprocess(obs) \n",
    "        \n",
    "        # Frame delta newframe - old frame\n",
    "        frame_delta = obs - self.previous_frame\n",
    "        #print(frame_delta.shape)\n",
    "        self.previous_frame = obs \n",
    "        \n",
    "        # Reshape the reward function ################################################################## This is where we adjust the game reward\n",
    "        score_delta = info['score'] - self.score \n",
    "        health_delta = info['health'] - self.health\n",
    "        enemy_health_delta = info['enemy_health'] - self.enemy_health\n",
    "        reward = score_delta + 50*health_delta - 50*enemy_health_delta\n",
    "        # add 500 for win - 500 for loss \n",
    "        if (self.matches_won != info['matches_won'] and info['matches_won'] != 0):\n",
    "            reward = reward + 500\n",
    "        if (self.enemy_matches_won != info['enemy_matches_won'] and info['enemy_matches_won'] != 0):\n",
    "            reward = reward - 500\n",
    "        \n",
    "        #reward function losing health is bad hurting the enemy is good, getting points is good\n",
    "        self.score = info['score'] \n",
    "        self.health = info['health']\n",
    "        self.enemy_health = info['enemy_health']\n",
    "        self.matches_won = info['matches_won']\n",
    "        self.enemy_matches_won = info['enemy_matches_won']\n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "    #functions for rendering the game as an mp4 and for closing the game\n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfGoGCm4k2hR"
   },
   "source": [
    "#Testing Custom Environment space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "q2as0oeXmTsx"
   },
   "outputs": [],
   "source": [
    "#code to display results\n",
    "\n",
    "from base64 import b64encode\n",
    "def render_mp4(videopath: str) -> str:\n",
    "  mp4 = open(videopath, 'rb').read()\n",
    "  base64_encoded_mp4 = b64encode(mp4).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing the optimzation frame - HPO\n",
    "import optuna\n",
    "# PPO algo for RL. Proximal Policy Optimization shown to work better \n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Bring in the eval policy method for metric calculation\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Import the sb3 monitor for logging \n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# Import the vec wrappers to vectorize and frame stack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "# Import os to deal with filepaths\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directories to store models so that we can save each increment model\n",
    "LOG_DIR = './logs2/'\n",
    "OPT_DIR = './opt2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return test hyperparameters - define the object function\n",
    "#Optuna as a library allows us to suggest hyper parameters we would like to train our model with\n",
    "# these 5 hyper parameters are the parameters needed for the PPO (Proximal Policy Optimization Algorithm)\n",
    "# Through some research we read the DQN provided unstable results, and this was shown in what we got from it\n",
    "# PPO is also less memory intensive and should train afaster\n",
    "def optimize_ppo(trial): \n",
    "    return {\n",
    "        #ppo wants steps to be a multiple of 64 to work well\n",
    "        'n_steps':trial.suggest_int('n_steps', 2048, 8192),\n",
    "        'gamma':trial.suggest_loguniform('gamma', 0.8, 0.9999),\n",
    "        'learning_rate':trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
    "        'clip_range':trial.suggest_uniform('clip_range', 0.1, 0.4),\n",
    "        'gae_lambda':trial.suggest_uniform('gae_lambda', 0.8, 0.99)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_modelv2.1'.format(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ19kMBnQfOF"
   },
   "source": [
    "## 3 Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base callback \n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#where we save the model, we save a model every 10000 steps\n",
    "CHECKPOINT_DIR = './train2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment \n",
    "env.close()\n",
    "\n",
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set model\n",
    "model = PPO.load('best_model_5180000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  0,   0,   0,  36],\n",
       "         [  0,   0,   0,  36],\n",
       "         [  0,   0,   0, 155],\n",
       "         ...,\n",
       "         [  0,   0,   0,  62],\n",
       "         [  0,   0,   0,  34],\n",
       "         [  0,   0,   0,  38]],\n",
       "\n",
       "        [[  0,   0,   0,  36],\n",
       "         [  0,   0,   0,  40],\n",
       "         [  0,   0,   0,  61],\n",
       "         ...,\n",
       "         [  0,   0,   0,  30],\n",
       "         [  0,   0,   0,  36],\n",
       "         [  0,   0,   0,  36]],\n",
       "\n",
       "        [[  0,   0,   0,  36],\n",
       "         [  0,   0,   0,  36],\n",
       "         [  0,   0,   0,  14],\n",
       "         ...,\n",
       "         [  0,   0,   0,  32],\n",
       "         [  0,   0,   0,  39],\n",
       "         [  0,   0,   0,  36]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0,   0,   0, 156],\n",
       "         [  0,   0,   0, 204],\n",
       "         [  0,   0,   0, 170],\n",
       "         ...,\n",
       "         [  0,   0,   0, 170],\n",
       "         [  0,   0,   0, 204],\n",
       "         [  0,   0,   0, 190]],\n",
       "\n",
       "        [[  0,   0,   0, 162],\n",
       "         [  0,   0,   0, 162],\n",
       "         [  0,   0,   0, 162],\n",
       "         ...,\n",
       "         [  0,   0,   0, 162],\n",
       "         [  0,   0,   0, 162],\n",
       "         [  0,   0,   0, 162]],\n",
       "\n",
       "        [[  0,   0,   0, 162],\n",
       "         [  0,   0,   0, 162],\n",
       "         [  0,   0,   0, 162],\n",
       "         ...,\n",
       "         [  0,   0,   0, 162],\n",
       "         [  0,   0,   0, 162],\n",
       "         [  0,   0,   0, 162]]]], dtype=uint8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset game to starting state\n",
    "obs = env.reset()\n",
    "# Set flag to flase\n",
    "done = False\n",
    "for game in range(1): \n",
    "    while not done: \n",
    "        if done: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.01)\n",
    "        #print(info)\n",
    "env.reset()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'enemy_matches_won': 2,\n",
       "  'score': 38800,\n",
       "  'matches_won': 0,\n",
       "  'continuetimer': 10,\n",
       "  'enemy_health': 0,\n",
       "  'health': 0,\n",
       "  'episode': {'r': 38800, 'l': 13743, 't': 689.282573},\n",
       "  'terminal_observation': array([[[0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0]]], dtype=uint8)}]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:downgrade] *",
   "language": "python",
   "name": "conda-env-downgrade-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
